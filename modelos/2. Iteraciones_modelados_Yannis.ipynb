{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVnvyjZc2kSS0rk4i3m6Sm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["---\n","\n","## CARGA DE LIBRER√çA Y DATOS\n","\n","---"],"metadata":{"id":"5L1ZoNAIKkh2"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XZRUejqKj4m","executionInfo":{"status":"ok","timestamp":1764773730118,"user_tz":-60,"elapsed":54283,"user":{"displayName":"Yannis German Saura","userId":"11705243329343073633"}},"outputId":"6cb5c4d0-951a-497f-aaf4-7eaa2d394740"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcai8MQjJ9vy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import math\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import f1_score, classification_report\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","from scipy.stats import chi2\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import KNNImputer\n","from sklearn.preprocessing import FunctionTransformer, RobustScaler\n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/Cupido_IA_project/train.csv')\n","test = pd.read_csv(\"/content/drive/MyDrive/Cupido_IA_project/test.csv\")\n","submission = pd.read_csv(\"/content/drive/MyDrive/Cupido_IA_project/sample_submission.csv\")"],"metadata":{"id":"0EikpVDqKoq_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## DEFINICI√ìN PREPROCESADO\n","\n","---"],"metadata":{"id":"lMNPnCy9LT3B"}},{"cell_type":"code","source":["cols_to_int = ['age', 'sex', 'cp', 'restecg']\n","\n","rename_dict = {\n","    \"age\": \"edad\",\n","    \"sex\": \"sexo\",\n","    \"cp\": \"tipo_dolor_pecho\",\n","    \"trestbps\": \"tension_en_descanso\",\n","    \"chol\": \"colesterol\",\n","    \"fbs\": \"azucar\",\n","    \"restecg\": \"electro_en_descanso\",\n","    \"thalach\": \"latidos_por_minuto\",\n","    \"exang\": \"dolor_pecho_con_ejercicio\",\n","    \"oldpeak\": \"cambio_linea_corazon_ejercicio\",\n","    \"slope\": \"forma_linea_corazon_ejercicio\",\n","    \"ca\": \"num_venas_grandes\",\n","    \"thal\": \"estado_corazon_thal\" }\n","\n","cols_a_clippear = [\n","    'tension_en_descanso', 'colesterol',\n","    'latidos_por_minuto', 'cambio_linea_corazon_ejercicio']\n","\n","categorical_cols_to_round = [\n","    'num_venas_grandes', 'estado_corazon_thal', 'sexo',\n","    'tipo_dolor_pecho', 'dolor_pecho_con_ejercicio',\n","    'azucar', 'forma_linea_corazon_ejercicio', 'electro_en_descanso']"],"metadata":{"id":"kLy152-tLTJC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","FUNCIONES DE PREPROCESADO\n","\n","---"],"metadata":{"id":"ktZhfHfSMWqF"}},{"cell_type":"code","source":["def limpieza_inicial(df):\n","    \"\"\"\n","    Realiza conversiones de tipos, renombres y limpieza b√°sica de errores (-9).\n","    Se puede aplicar a todo el dataset antes del split.\n","    \"\"\"\n","    df = df.copy()\n","\n","    # Conversi√≥n a Int\n","    for col in cols_to_int:\n","        if col in df.columns:\n","            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n","\n","    # Conversi√≥n de objetos a num√©rico\n","    object_cols = df.select_dtypes(include=['object']).columns\n","    for col in object_cols:\n","        df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","    # Renombrar\n","    df = df.rename(columns=rename_dict)\n","\n","    # Reemplazar errores conocidos (-9) por NaN\n","    df.replace([-9, -9.0], np.nan, inplace=True)\n","\n","    return df"],"metadata":{"id":"yZYIcR-sMM5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def limpiar_ceros_fisiologicos(X):\n","    X = X.copy()\n","    cols_imposibles_con_cero = ['tension_en_descanso', 'colesterol']\n","    for col in cols_imposibles_con_cero:\n","        if col in X.columns:\n","            X[col] = X[col].replace({0: np.nan, 0.0: np.nan})\n","    return X"],"metadata":{"id":"k6WPY8AxMtgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clipear_outliers(X):\n","    \"\"\"\n","    Nota: Usado en FunctionTransformer, esto calcular√° los cuantiles\n","    sobre el lote actual de datos.\n","    \"\"\"\n","    X = X.copy()\n","    for col in cols_a_clippear:\n","        if col in X.columns:\n","            p1 = X[col].quantile(0.01)\n","            p99 = X[col].quantile(0.99)\n","            X[col] = X[col].clip(lower=p1, upper=p99)\n","    return X"],"metadata":{"id":"BccdJqYGMwz8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def crear_flags_mnar(df):\n","    df_new = df.copy()\n","    cols_mnar = ['num_venas_grandes', 'estado_corazon_thal']\n","    for col in cols_mnar:\n","        if col in df_new.columns:\n","            df_new[f'{col}_is_missing'] = df_new[col].isna().astype(int)\n","    return df_new"],"metadata":{"id":"w7g5ipXWM0Uf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def redondear_imputaciones(X):\n","    X = X.copy()\n","    for col in categorical_cols_to_round:\n","        if col in X.columns:\n","            X[col] = X[col].round()\n","    return X"],"metadata":{"id":"z8UXyjD_M3K2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimizar_k_knn(X_train, k_range=[3, 5, 7, 9, 11, 15]):\n","    # Preparamos una copia limpia para testear\n","    X_temp = limpiar_ceros_fisiologicos(X_train)\n","    X_temp = clipear_outliers(X_temp)\n","    # Solo usamos filas completas para validar el error de imputaci√≥n\n","    X_complete = X_temp.dropna().copy()\n","\n","    if len(X_complete) < 50:\n","        print(\"Pocos datos completos. Se usar√° k=5 por defecto.\")\n","        return 5\n","\n","    rmse_scores = {}\n","    scaler = RobustScaler()\n","    X_scaled_array = scaler.fit_transform(X_complete)\n","\n","    np.random.seed(42)\n","    mask = np.random.rand(*X_scaled_array.shape) < 0.1\n","    X_missing_sim = X_scaled_array.copy()\n","    X_missing_sim[mask] = np.nan\n","\n","    print(f\"Buscando k √≥ptimo sobre {len(X_complete)} muestras...\")\n","    for k in k_range:\n","        imputer = KNNImputer(n_neighbors=k, weights='distance')\n","        X_imputed = imputer.fit_transform(X_missing_sim)\n","        error = np.sqrt(mean_squared_error(X_scaled_array[mask], X_imputed[mask]))\n","        rmse_scores[k] = error\n","\n","    best_k = min(rmse_scores, key=rmse_scores.get)\n","    print(f\"Mejor k encontrado: {best_k}\")\n","    return best_k"],"metadata":{"id":"AfPbT_3eM9FR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## APLICACI√ìN DEL FLUJO DE PREPROCESADO DEFINIDO\n","\n","---"],"metadata":{"id":"dz9Jv11cM-F-"}},{"cell_type":"code","source":["df_train = df.copy()\n","df_test = test.copy()\n","\n","df_train = limpieza_inicial(df_train)\n","df_test = limpieza_inicial(df_test)\n","\n","target = \"label\"\n","\n","X_train = df_train.drop(columns=target)\n","y_train = df_train[target]\n","\n","if target in df_test.columns:\n","    X_test = df_test.drop(columns=target)\n","    y_test = df_test[target]\n","else:\n","    X_test = df_test.copy()\n","\n","best_k = optimizar_k_knn(X_train)\n","\n","pipeline_imputacion = Pipeline([\n","    ('limpieza_ceros', FunctionTransformer(limpiar_ceros_fisiologicos, validate=False)),\n","    ('clipear_outliers', FunctionTransformer(clipear_outliers, validate=False)),\n","    ('mnar_flags', FunctionTransformer(crear_flags_mnar, validate=False)),\n","    ('scaler', RobustScaler()), # Aprende la mediana y rango intercuart√≠lico de TRAIN\n","    ('knn_imputer', KNNImputer(n_neighbors=best_k, weights='distance')), # Aprende vecinos de TRAIN\n","    ('rounding', FunctionTransformer(redondear_imputaciones, validate=False))\n","]).set_output(transform=\"pandas\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FR0YZtJqNFii","executionInfo":{"status":"ok","timestamp":1764773757608,"user_tz":-60,"elapsed":11,"user":{"displayName":"Yannis German Saura","userId":"11705243329343073633"}},"outputId":"00f0297d-0d63-4218-8177-6abd893ff255"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Buscando k √≥ptimo sobre 246 muestras...\n","Mejor k encontrado: 15\n"]}]},{"cell_type":"markdown","source":["---\n","## EJECUCI√ìN Y VERIFICACI√ìN DE TRANSFORMACI√ìN\n","\n","---"],"metadata":{"id":"Ojh42bcSOFnS"}},{"cell_type":"code","source":["print(\"Ajustando pipeline con TRAIN completo...\")\n","X_train_prep = pipeline_imputacion.fit_transform(X_train)\n","\n","print(\"Aplicando preprocesamiento a TEST...\")\n","X_test_prep = pipeline_imputacion.transform(X_test)\n","\n","print(\"\\n--- Proceso finalizado ---\")\n","print(f\"Dimensiones Train procesado: {X_train_prep.shape}\")\n","print(f\"Dimensiones Test procesado : {X_test_prep.shape}\")\n","\n","print(f\"Nulos restantes en Train: {X_train_prep.isna().sum().sum()}\")\n","print(f\"Nulos restantes en Test : {X_test_prep.isna().sum().sum()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TdCZCR6DOJ-U","executionInfo":{"status":"ok","timestamp":1764773758211,"user_tz":-60,"elapsed":300,"user":{"displayName":"Yannis German Saura","userId":"11705243329343073633"}},"outputId":"b05423ac-df0c-4096-ef6b-80f18d40e58d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ajustando pipeline con TRAIN completo...\n","Aplicando preprocesamiento a TEST...\n","\n","--- Proceso finalizado ---\n","Dimensiones Train procesado: (732, 15)\n","Dimensiones Test procesado : (184, 15)\n","Nulos restantes en Train: 0\n","Nulos restantes en Test : 0\n"]}]},{"cell_type":"markdown","source":["---\n","## ENTRENAMIENTO Y PREDICCI√ìN SOBRE TEST CON SVM\n","\n","---"],"metadata":{"id":"o-nGdVafPxsH"}},{"cell_type":"code","source":["#SVM = 0.549\n","\n","from sklearn.svm import SVC\n","\n","print(\"Entrenando SVM con el dataset PREPROCESADO...\")\n","svm_final = SVC(probability=True, random_state=42)\n","\n","svm_final.fit(X_train_prep, y_train)\n","\n","print(\"Generando predicciones sobre el Test...\")\n","y_test_pred = svm_final.predict(X_test_prep)\n","\n","\n","# Verificaciones de seguridad\n","print(f\"Predicciones generadas: {len(y_test_pred)}\")\n","print(f\"Filas en sample_submission: {len(submission)}\")\n","\n","if len(y_test_pred) == len(submission):\n","    submission[\"label\"] = y_test_pred\n","    submission.to_csv(\"submission_svm_full_5.csv\", index=False)\n","    print(\"¬°Archivo 'submission_svm_full_5.csv' guardado con √©xito!\")\n","\n","    # Vista previa\n","    print(submission.head())\n","else:\n","    print(\"¬°ALERTA! Las dimensiones no coinciden. Revisa si se borraron filas en el test.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KeVIN3LvP6r6","executionInfo":{"status":"ok","timestamp":1764766026983,"user_tz":-60,"elapsed":723,"user":{"displayName":"Yannis German Saura","userId":"11705243329343073633"}},"outputId":"0a66aef6-4f9a-4ae4-b6ba-002cba09ad3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrenando SVM con el dataset PREPROCESADO...\n","Generando predicciones sobre el Test...\n","Predicciones generadas: 184\n","Filas en sample_submission: 184\n","¬°Archivo 'submission_svm_full_5.csv' guardado con √©xito!\n","   ID  label\n","0   0      3\n","1   1      0\n","2   2      0\n","3   3      2\n","4   4      0\n"]}]},{"cell_type":"markdown","source":["---\n","## ENTRENAMIENTO Y PREDICCI√ìN SOBRE TEST CON GRADIENT BOOSTING\n","\n","---"],"metadata":{"id":"uwtXax-rQdV2"}},{"cell_type":"code","source":["#Mejoro un 0.1 con preprocesado avanzado (posible descarte)\n","\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","print(\"Entrenando Gradient Boosting con el dataset PREPROCESADO...\")\n","\n","gb_final = GradientBoostingClassifier(random_state=42)\n","gb_final.fit(X_train_prep, y_train)\n","\n","print(\"Generando predicciones sobre el Test...\")\n","y_test_pred = gb_final.predict(X_test_prep)\n","\n","# Verificaciones de seguridad\n","print(f\"Predicciones generadas: {len(y_test_pred)}\")\n","print(f\"Filas en sample_submission: {len(submission)}\")\n","\n","if len(y_test_pred) == len(submission):\n","    submission[\"label\"] = y_test_pred\n","    # Cambiamos el nombre del archivo para distinguirlo del SVM\n","    submission.to_csv(\"submission_gb_full_6.csv\", index=False)\n","    print(\"¬°Archivo 'submission_gb_full_6.csv' guardado con √©xito!\")\n","\n","    # Vista previa\n","    print(submission.head())\n","else:\n","    print(\"¬°ALERTA! Las dimensiones no coinciden. Revisa si se borraron filas en el test.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xaqFfRH9QlmZ","executionInfo":{"status":"ok","timestamp":1764766243653,"user_tz":-60,"elapsed":2340,"user":{"displayName":"Yannis German Saura","userId":"11705243329343073633"}},"outputId":"fb45a5b5-f032-4546-aa01-1241f698b83a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrenando Gradient Boosting con el dataset PREPROCESADO...\n","Generando predicciones sobre el Test...\n","Predicciones generadas: 184\n","Filas en sample_submission: 184\n","¬°Archivo 'submission_gb_full_6.csv' guardado con √©xito!\n","   ID  label\n","0   0      4\n","1   1      0\n","2   2      0\n","3   3      1\n","4   4      0\n"]}]},{"cell_type":"markdown","source":["---\n","## ENTRENAMIENTO Y PREDICCI√ìN SOBRE TEST CON REGRESI√ìN LOG√çSTICA\n","\n","---"],"metadata":{"id":"r4XkTG4jRcKU"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","print(\"Entrenando Regresi√≥n Log√≠stica con el dataset PREPROCESADO...\")\n","\n","lr_final = LogisticRegression(random_state=42, max_iter=1000)\n","lr_final.fit(X_train_prep, y_train)\n","\n","print(\"Generando predicciones sobre el Test...\")\n","y_test_pred = lr_final.predict(X_test_prep)\n","\n","# Verificaciones de seguridad\n","print(f\"Predicciones generadas: {len(y_test_pred)}\")\n","print(f\"Filas en sample_submission: {len(submission)}\")\n","\n","if len(y_test_pred) == len(submission):\n","    submission[\"label\"] = y_test_pred\n","    # Nombre de archivo actualizado\n","    submission.to_csv(\"submission_logreg_full_7.csv\", index=False)\n","    print(\"¬°Archivo 'submission_logreg_full_7.csv' guardado con √©xito!\")\n","\n","    # Vista previa\n","    print(submission.head())\n","else:\n","    print(\"¬°ALERTA! Las dimensiones no coinciden.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIqtToGtRggJ","executionInfo":{"status":"ok","timestamp":1764766448437,"user_tz":-60,"elapsed":149,"user":{"displayName":"Yannis German Saura","userId":"11705243329343073633"}},"outputId":"6fe9d3c3-2d29-44cd-9433-b466beece4a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrenando Regresi√≥n Log√≠stica con el dataset PREPROCESADO...\n","Generando predicciones sobre el Test...\n","Predicciones generadas: 184\n","Filas en sample_submission: 184\n","¬°Archivo 'submission_logreg_full_7.csv' guardado con √©xito!\n","   ID  label\n","0   0      4\n","1   1      0\n","2   2      0\n","3   3      2\n","4   4      0\n"]}]},{"cell_type":"markdown","source":["---\n","## ENTRENAMIENTO Y PREDICCI√ìN SOBRE TEST CON GRADIENT CLASSIFIER\n","\n","---"],"metadata":{"id":"gsHV8VzQSCyl"}},{"cell_type":"code","source":["# modelos de familia boosting descartado\n","\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","print(\"Entrenando Gradient Boosting con el dataset PREPROCESADO...\")\n","gb_final = GradientBoostingClassifier(random_state=42)\n","gb_final.fit(X_train_prep, y_train)\n","\n","print(\"Generando predicciones sobre el Test...\")\n","y_test_pred = gb_final.predict(X_test_prep)\n","\n","# Verificaciones de seguridad\n","print(f\"Predicciones generadas: {len(y_test_pred)}\")\n","print(f\"Filas en sample_submission: {len(submission)}\")\n","\n","if len(y_test_pred) == len(submission):\n","    submission[\"label\"] = y_test_pred\n","    # Guardamos con un nombre descriptivo\n","    submission.to_csv(\"submission_gradient_boosting_full.csv\", index=False)\n","    print(\"¬°Archivo 'submission_gradient_boosting_full.csv' guardado con √©xito!\")\n","\n","    # Vista previa\n","    print(submission.head())\n","else:\n","    print(\"¬°ALERTA! Las dimensiones no coinciden.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXmktSn6SBP_","executionInfo":{"status":"ok","timestamp":1764766623245,"user_tz":-60,"elapsed":2003,"user":{"displayName":"Yannis German Saura","userId":"11705243329343073633"}},"outputId":"6c037093-e116-4c69-dcbb-7a9ef230a317"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrenando Gradient Boosting con el dataset PREPROCESADO...\n","Generando predicciones sobre el Test...\n","Predicciones generadas: 184\n","Filas en sample_submission: 184\n","¬°Archivo 'submission_gradient_boosting_full.csv' guardado con √©xito!\n","   ID  label\n","0   0      4\n","1   1      0\n","2   2      0\n","3   3      1\n","4   4      0\n"]}]},{"cell_type":"markdown","source":["---\n","## DOCUMENTACI√ìN\n","\n","---"],"metadata":{"id":"ZY9coeWqMpBB"}},{"cell_type":"markdown","source":["En esta fase se busc√≥ romper el \"techo de cristal\" de la iteraci√≥n anterior mediante una ingenier√≠a de datos avanzada. El resultado fue un √©xito rotundo para los modelos sensibles a la escala, validando la hip√≥tesis de que el ruido era el principal limitante.\n","\n","### 1. üõ†Ô∏è Refinamiento del Pipeline (La Clave del √âxito)\n","Se abandon√≥ la imputaci√≥n simple y se implement√≥ un pipeline robusto dise√±ado para limpiar la se√±al:\n","* **Limpieza Fisiol√≥gica:** Correcci√≥n de valores imposibles (ej. ceros en colesterol) antes de imputar.\n","* **Manejo de Outliers:** Clipping y uso de `RobustScaler` (basado en cuartiles) en lugar de StandardScaler (basado en media/desviaci√≥n), eliminando la distorsi√≥n por valores extremos.\n","* **Imputaci√≥n Avanzada (KNN):** Uso de `KNNImputer` (k=15) para reconstruir datos faltantes bas√°ndose en la similitud entre pacientes reales.\n","\n","### 2. üèÜ Resultados en Kaggle (Impacto del Preprocesado)\n","Los resultados validaron la estrategia de limpieza agresiva:\n","\n","* **ü•á SVM (0.55978):** **Salto masivo de rendimiento.** Pas√≥ de ser el peor modelo (0.35 en It. 1) a ser el **mejor**. Esto confirma que el algoritmo era correcto, pero los datos \"sucios\" anteriores le imped√≠an trazar un hiperplano v√°lido.\n","* **ü•à Regresi√≥n Log√≠stica (0.54347):** Mantuvo su solidez y mejor√≥ gracias a la imputaci√≥n KNN, confirmando la fuerte componente lineal del problema.\n","* **ü•â Gradient Boosting (0.49456):** Se estanc√≥. Al ser un modelo robusto por naturaleza a outliers y nulos, no se benefici√≥ tanto de la limpieza avanzada como sus rivales, quedando relegado.\n","\n","### 3. üìä Observaciones Cr√≠ticas\n","* **El Preprocesado es el Modelo:** La mejora de +0.20 en el SVM demuestra que en este dataset, la ingenier√≠a de caracter√≠sticas y la limpieza aportan m√°s valor que la complejidad del algoritmo.\n","* **Validaci√≥n de Robustez:** `RobustScaler` fue el factor diferencial. Al ignorar los outliers en el escalado, permiti√≥ que el SVM y la Regresi√≥n Log√≠stica vieran la distribuci√≥n real de los datos.\n","\n","### 4. üöÄ Pr√≥ximos Pasos\n","Con un SVM fuerte (0.56) y un Gradient Boosting estancado (0.49), la estrategia natural es el **Ensamblaje (Stacking)**. Se buscar√° combinar la precisi√≥n geom√©trica del SVM con la capacidad de los √°rboles para capturar excepciones no lineales, intentando superar la barrera del 0.57."],"metadata":{"id":"wq_5kxM-Imyz"}}]}