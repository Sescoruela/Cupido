{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## CARGA DE LIBRER√çA Y DATOS\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5L1ZoNAIKkh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XZRUejqKj4m",
        "outputId": "aafd3264-73f0-4d94-962c-e2e7088b6599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcai8MQjJ9vy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy.stats import chi2, f_oneway,chi2_contingency\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, RobustScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Cupido_IA_project/train.csv')\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Cupido_IA_project/test.csv\")\n",
        "submission = pd.read_csv(\"/content/drive/MyDrive/Cupido_IA_project/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "0EikpVDqKoq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## DEFINICI√ìN PREPROCESADO\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lMNPnCy9LT3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_int = ['age', 'sex', 'cp', 'restecg']\n",
        "\n",
        "rename_dict = {\n",
        "    \"age\": \"edad\",\n",
        "    \"sex\": \"sexo\",\n",
        "    \"cp\": \"tipo_dolor_pecho\",\n",
        "    \"trestbps\": \"tension_en_descanso\",\n",
        "    \"chol\": \"colesterol\",\n",
        "    \"fbs\": \"azucar\",\n",
        "    \"restecg\": \"electro_en_descanso\",\n",
        "    \"thalach\": \"latidos_por_minuto\",\n",
        "    \"exang\": \"dolor_pecho_con_ejercicio\",\n",
        "    \"oldpeak\": \"cambio_linea_corazon_ejercicio\",\n",
        "    \"slope\": \"forma_linea_corazon_ejercicio\",\n",
        "    \"ca\": \"num_venas_grandes\",\n",
        "    \"thal\": \"estado_corazon_thal\"\n",
        "}\n",
        "\n",
        "cols_a_clippear = [\n",
        "    'tension_en_descanso', 'colesterol',\n",
        "    'latidos_por_minuto', 'cambio_linea_corazon_ejercicio'\n",
        "]\n",
        "\n",
        "categorical_cols_to_round = [\n",
        "    'num_venas_grandes', 'estado_corazon_thal', 'sexo',\n",
        "    'tipo_dolor_pecho', 'dolor_pecho_con_ejercicio',\n",
        "    'azucar', 'forma_linea_corazon_ejercicio', 'electro_en_descanso'\n",
        "]"
      ],
      "metadata": {
        "id": "kLy152-tLTJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "FUNCIONES DE PREPROCESADO\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ktZhfHfSMWqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpieza_inicial(df):\n",
        "    \"\"\"\n",
        "    Realiza conversiones de tipos, renombres y limpieza b√°sica de errores (-9).\n",
        "    Se puede aplicar a todo el dataset antes del split.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    for col in cols_to_int:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
        "\n",
        "    object_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in object_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    df.replace([-9, -9.0], np.nan, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "yZYIcR-sMM5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_ceros_fisiologicos(X):\n",
        "    X = X.copy()\n",
        "    cols_imposibles_con_cero = ['tension_en_descanso', 'colesterol']\n",
        "    for col in cols_imposibles_con_cero:\n",
        "        if col in X.columns:\n",
        "            X[col] = X[col].replace({0: np.nan, 0.0: np.nan})\n",
        "    return X"
      ],
      "metadata": {
        "id": "k6WPY8AxMtgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clipear_outliers(X):\n",
        "    X = X.copy()\n",
        "    for col in cols_a_clippear:\n",
        "        if col in X.columns:\n",
        "            p1 = X[col].quantile(0.01)\n",
        "            p99 = X[col].quantile(0.99)\n",
        "            X[col] = X[col].clip(lower=p1, upper=p99)\n",
        "    return X"
      ],
      "metadata": {
        "id": "BccdJqYGMwz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_flags_mnar(df):\n",
        "    \"\"\"\n",
        "    ACTUALIZADO (Estrategia Pruning):\n",
        "    Solo creamos flag para 'num_venas_grandes'.\n",
        "    'estado_corazon_thal_is_missing' se considera ruido (Grupo 1) y no se genera.\n",
        "    \"\"\"\n",
        "    df_new = df.copy()\n",
        "    # Solo venas, thal is missing se elimina por V=0.040\n",
        "    cols_mnar = ['num_venas_grandes']\n",
        "    for col in cols_mnar:\n",
        "        if col in df_new.columns:\n",
        "            df_new[f'{col}_is_missing'] = df_new[col].isna().astype(int)\n",
        "    return df_new"
      ],
      "metadata": {
        "id": "w7g5ipXWM0Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RobustKNNImputerWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.scaler = RobustScaler()\n",
        "        self.imputer = KNNImputer(n_neighbors=n_neighbors, weights='distance')\n",
        "        self.feature_names_in_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.feature_names_in_ = X.columns if hasattr(X, 'columns') else [f\"feat_{i}\" for i in range(X.shape[1])]\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        self.imputer.fit(X_scaled)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        X_imputed_scaled = self.imputer.transform(X_scaled)\n",
        "        X_imputed = self.scaler.inverse_transform(X_imputed_scaled)\n",
        "        return pd.DataFrame(X_imputed, columns=self.feature_names_in_, index=X.index)\n",
        "\n",
        "    def set_output(self, *, transform=None):\n",
        "        return self"
      ],
      "metadata": {
        "id": "tcGtUVuz-Ycf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def redondear_imputaciones(X):\n",
        "    X = X.copy()\n",
        "    for col in categorical_cols_to_round:\n",
        "        if col in X.columns:\n",
        "            X[col] = X[col].round()\n",
        "    return X"
      ],
      "metadata": {
        "id": "k6FNu_mFJaP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aplicar_feature_engineering_avanzado(df):\n",
        "    \"\"\"\n",
        "    ACTUALIZADO:\n",
        "    1. Se eliminan c√°lculos innecesarios (rango_colesterol, porcentaje_freq).\n",
        "    2. Se calculan las variables compuestas necesarias antes de borrar las fuentes.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. Flag Depresi√≥n ST (Usada en Score Stress)\n",
        "    if 'cambio_linea_corazon_ejercicio' in df.columns:\n",
        "        df['flag_depresion_st'] = (df['cambio_linea_corazon_ejercicio'] > 0).astype(int)\n",
        "\n",
        "    # 2. Flag Hipertensi√≥n (Absorbe info de tension_en_descanso)\n",
        "    if 'tension_en_descanso' in df.columns:\n",
        "        df['flag_hipertension'] = (df['tension_en_descanso'] > 130).astype(int)\n",
        "\n",
        "    # NOTA: Eliminado calculo de 'porcentaje_frecuencia_max' (Grupo 2 - Redundancia negativa)\n",
        "    # NOTA: Eliminado calculo de 'rango_colesterol' (Grupo 1 - Ruido)\n",
        "\n",
        "    # 3. Score Respuesta al Estr√©s (Driver Tier 1)\n",
        "    if 'dolor_pecho_con_ejercicio' in df.columns and 'flag_depresion_st' in df.columns:\n",
        "        df['score_respuesta_stress'] = df['dolor_pecho_con_ejercicio'] + df['flag_depresion_st']\n",
        "\n",
        "    # 4. Carga de Comorbilidad (Absorbe azucar y electro)\n",
        "    cols_comorbilidad = ['azucar', 'flag_hipertension', 'electro_en_descanso']\n",
        "    if set(cols_comorbilidad).issubset(df.columns):\n",
        "        # Nos aseguramos que electro sea binario para la suma (0 = normal, 1,2 = anormal)\n",
        "        electro_punto = (df['electro_en_descanso'] > 0).astype(int)\n",
        "        df['carga_comorbilidad'] = df['azucar'] + df['flag_hipertension'] + electro_punto\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "1nl8bYKh-fFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ejecutar_pruning_agresivo(df):\n",
        "    \"\"\"\n",
        "    Elimina las variables de Grupo 1 (Ruido) y Grupo 2 (Redundancia/Absorbidas)\n",
        "    despu√©s de haberlas utilizado para crear las variables compuestas.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    vars_a_eliminar = [\n",
        "        # GRUPO 1: Ruido Puro\n",
        "        'electro_en_descanso', # Usado en carga, adios.\n",
        "        'colesterol',          # Inutil en este dataset, adios.\n",
        "        # 'rango_colesterol' y 'thal_missing' ya no se generan.\n",
        "\n",
        "        # GRUPO 2: Redundantes / Absorbidas\n",
        "        'azucar',               # Absorbida en carga, adios.\n",
        "        'tension_en_descanso'   # Absorbida en flag_hipertension, adios.\n",
        "    ]\n",
        "\n",
        "    # Eliminamos solo si existen (para evitar errores)\n",
        "    cols_existentes = [c for c in vars_a_eliminar if c in df.columns]\n",
        "    if cols_existentes:\n",
        "        df = df.drop(columns=cols_existentes)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Did4d0XdJmfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizar_k_knn(X_train, k_range=[3, 5, 7, 9, 11, 15]):\n",
        "    # Versi√≥n simplificada de la original\n",
        "    X_temp = limpiar_ceros_fisiologicos(X_train)\n",
        "    X_temp = clipear_outliers(X_temp)\n",
        "    X_complete = X_temp.dropna().copy()\n",
        "    if len(X_complete) < 50: return 5\n",
        "\n",
        "    rmse_scores = {}\n",
        "    scaler = RobustScaler()\n",
        "    X_scaled_array = scaler.fit_transform(X_complete)\n",
        "    np.random.seed(42)\n",
        "    mask = np.random.rand(*X_scaled_array.shape) < 0.1\n",
        "    X_missing_sim = X_scaled_array.copy()\n",
        "    X_missing_sim[mask] = np.nan\n",
        "\n",
        "    for k in k_range:\n",
        "        imputer = KNNImputer(n_neighbors=k, weights='distance')\n",
        "        X_imputed = imputer.fit_transform(X_missing_sim)\n",
        "        error = np.sqrt(mean_squared_error(X_scaled_array[mask], X_imputed[mask]))\n",
        "        rmse_scores[k] = error\n",
        "\n",
        "    return min(rmse_scores, key=rmse_scores.get)"
      ],
      "metadata": {
        "id": "AfPbT_3eM9FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## APLICACI√ìN DEL FLUJO DE PREPROCESADO DEFINIDO\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dz9Jv11cM-F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df.copy()\n",
        "df_test = test.copy()\n",
        "\n",
        "df_train = limpieza_inicial(df_train)\n",
        "df_test = limpieza_inicial(df_test)\n",
        "\n",
        "target = \"label\"\n",
        "\n",
        "X_train = df_train.drop(columns=target)\n",
        "y_train = df_train[target]\n",
        "\n",
        "if target in df_test.columns:\n",
        "    X_test = df_test.drop(columns=target)\n",
        "    y_test = df_test[target]\n",
        "else:\n",
        "    X_test = df_test.copy()\n",
        "\n",
        "best_k = optimizar_k_knn(X_train)\n",
        "\n",
        "# 3. Definici√≥n del Pipeline\n",
        "pipeline_feature_engineering = Pipeline([\n",
        "    ('limpieza_ceros', FunctionTransformer(limpiar_ceros_fisiologicos, validate=False)),\n",
        "    ('clipear_outliers', FunctionTransformer(clipear_outliers, validate=False)),\n",
        "    ('mnar_flags', FunctionTransformer(crear_flags_mnar, validate=False)),\n",
        "    ('imputacion_robusta', RobustKNNImputerWrapper(n_neighbors=best_k)), # Usamos el K optimizado\n",
        "    ('rounding', FunctionTransformer(redondear_imputaciones, validate=False)),\n",
        "    ('feature_engineering', FunctionTransformer(aplicar_feature_engineering_avanzado, validate=False)),\n",
        "    # NUEVO PASO: Pruning\n",
        "    ('pruning_agresivo', FunctionTransformer(ejecutar_pruning_agresivo, validate=False)),\n",
        "    ('final_scaler', RobustScaler())\n",
        "]).set_output(transform=\"pandas\")"
      ],
      "metadata": {
        "id": "FR0YZtJqNFii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## EJECUCI√ìN Y VERIFICACI√ìN DE TRANSFORMACI√ìN\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Ojh42bcSOFnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ajustando pipeline con estrategia de Pruning Agresivo...\")\n",
        "X_train_prep = pipeline_feature_engineering.fit_transform(X_train)\n",
        "y_train_prep = y_train.copy()\n",
        "\n",
        "X_test_prep = pipeline_feature_engineering.transform(X_test)\n",
        "print(\"\\n--- Proceso finalizado ---\")\n",
        "print(f\"Dimensiones Train final: {X_train_prep.shape}\")\n",
        "print(f\"Columnas finales en el dataset: \\n{X_train_prep.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdCZCR6DOJ-U",
        "outputId": "d0afad26-3518-4f83-d45e-566926ad9bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ajustando pipeline con estrategia de Pruning Agresivo...\n",
            "\n",
            "--- Proceso finalizado ---\n",
            "Dimensiones Train final: (732, 14)\n",
            "Columnas finales en el dataset: \n",
            "['edad', 'sexo', 'tipo_dolor_pecho', 'latidos_por_minuto', 'dolor_pecho_con_ejercicio', 'cambio_linea_corazon_ejercicio', 'forma_linea_corazon_ejercicio', 'num_venas_grandes', 'estado_corazon_thal', 'num_venas_grandes_is_missing', 'flag_depresion_st', 'flag_hipertension', 'score_respuesta_stress', 'carga_comorbilidad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ENTRENAMIENTO Y PREDICCI√ìN SOBRE TEST CON SVM\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eCwc085PMoI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Entrenando SVM con el dataset PREPROCESADO...\")\n",
        "svm_final = SVC(probability=True, random_state=42)\n",
        "\n",
        "svm_final.fit(X_train_prep, y_train)\n",
        "\n",
        "print(\"Generando predicciones sobre el Test...\")\n",
        "y_test_pred = svm_final.predict(X_test_prep)\n",
        "\n",
        "\n",
        "# Verificaciones de seguridad\n",
        "print(f\"Predicciones generadas: {len(y_test_pred)}\")\n",
        "print(f\"Filas en sample_submission: {len(submission)}\")\n",
        "\n",
        "if len(y_test_pred) == len(submission):\n",
        "    submission[\"label\"] = y_test_pred\n",
        "    submission.to_csv(\"submission_svm_pruned_1.csv\", index=False)\n",
        "    print(\"¬°Archivo 'submission_svm_pruned_1.csv' guardado con √©xito!\")\n",
        "\n",
        "    # Vista previa\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"¬°ALERTA! Las dimensiones no coinciden. Revisa si se borraron filas en el test.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzMbGVzFMnjG",
        "outputId": "524ee570-6c14-4484-986c-adb821b42a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando SVM con el dataset PREPROCESADO...\n",
            "Generando predicciones sobre el Test...\n",
            "Predicciones generadas: 184\n",
            "Filas en sample_submission: 184\n",
            "¬°Archivo 'submission_svm_pruned_1.csv' guardado con √©xito!\n",
            "   ID  label\n",
            "0   0      2\n",
            "1   1      0\n",
            "2   2      0\n",
            "3   3      2\n",
            "4   4      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "APLICACI√ìN DE HIPERPAR√ÅMETROS √ìPTIMOS SVM\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jN0ouN26csEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.56521\n",
        "print(\"Entrenando SVM con hiperpar√°metros OPTIMIZADOS (C=1, gamma=0.05)...\")\n",
        "\n",
        "svm_final = SVC(\n",
        "    C=1,\n",
        "    gamma=0.05,\n",
        "    kernel='rbf',\n",
        "    class_weight=None,\n",
        "    probability=True,\n",
        "    random_state=42)\n",
        "\n",
        "svm_final.fit(X_train_prep, y_train)\n",
        "\n",
        "print(\"Generando predicciones sobre el Test...\")\n",
        "y_test_pred = svm_final.predict(X_test_prep)\n",
        "\n",
        "\n",
        "# Verificaciones de seguridad\n",
        "print(f\"Predicciones generadas: {len(y_test_pred)}\")\n",
        "print(f\"Filas en sample_submission: {len(submission)}\")\n",
        "\n",
        "if len(y_test_pred) == len(submission):\n",
        "    submission[\"label\"] = y_test_pred\n",
        "    submission.to_csv(\"submission_svm_pruned_opt_1.csv\", index=False)\n",
        "    print(\"¬°Archivo 'submission_svm_pruned_opt_1.csv' guardado con √©xito!\")\n",
        "\n",
        "    # Vista previa\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"¬°ALERTA! Las dimensiones no coinciden. Revisa si se borraron filas en el test.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW3zcik6cwn5",
        "outputId": "fc621dac-c165-4e2e-9a1e-18452bf043a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando SVM con hiperpar√°metros OPTIMIZADOS (C=1, gamma=0.05)...\n",
            "Generando predicciones sobre el Test...\n",
            "Predicciones generadas: 184\n",
            "Filas en sample_submission: 184\n",
            "¬°Archivo 'submission_svm_pruned_opt_1.csv' guardado con √©xito!\n",
            "   ID  label\n",
            "0   0      3\n",
            "1   1      0\n",
            "2   2      0\n",
            "3   3      2\n",
            "4   4      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2¬∫ APLICACI√ìN DE HIPERPAR√ÅMETROS √ìPTIMOS SVM (empeora la anterior en kaggle)"
      ],
      "metadata": {
        "id": "TJrDj64qgTeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.54347\n",
        "print(\"--- Entrenando SVM REFINADO (C=1.1, gamma=0.04) ---\")\n",
        "\n",
        "# Configuraci√≥n ganadora del √∫ltimo GridSearch\n",
        "svm_refined = SVC(\n",
        "    C=1.1,\n",
        "    gamma=0.04,\n",
        "    kernel='rbf',\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "svm_refined.fit(X_train_prep, y_train)\n",
        "\n",
        "print(\"Generando predicciones finales...\")\n",
        "y_test_refined = svm_refined.predict(X_test_prep)\n",
        "\n",
        "if len(y_test_refined) == len(submission):\n",
        "    submission[\"label\"] = y_test_pred\n",
        "    submission.to_csv(\"submission_svm_pruned_opt_2.csv\", index=False)\n",
        "    print(\"¬°Archivo 'submission_svm_pruned_opt_2.csv' guardado con √©xito!\")\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"¬°ALERTA! Las dimensiones no coinciden. Revisa si se borraron filas en el test.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEOcKjyWgSrx",
        "outputId": "05d836e1-acd1-4a9c-b4aa-fdb6912ab69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Entrenando SVM REFINADO (C=1.1, gamma=0.04) ---\n",
            "Generando predicciones finales...\n",
            "¬°Archivo 'submission_svm_pruned_opt_2.csv' guardado con √©xito!\n",
            "   ID  label\n",
            "0   0      3\n",
            "1   1      0\n",
            "2   2      0\n",
            "3   3      2\n",
            "4   4      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ENTRENAMIENTO Y PREDICCI√ìN SOBRE TEST CON REGRESI√ìN LOG√çSTICA\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yCLcsNnwM-EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Entrenando Regresi√≥n Log√≠stica con el dataset PREPROCESADO...\")\n",
        "\n",
        "lr_final = LogisticRegression(random_state=42, max_iter=1000,class_weight='balanced')\n",
        "lr_final.fit(X_train_prep, y_train)\n",
        "\n",
        "print(\"Generando predicciones sobre el Test...\")\n",
        "y_test_pred = lr_final.predict(X_test_prep)\n",
        "\n",
        "# Verificaciones de seguridad\n",
        "print(f\"Predicciones generadas: {len(y_test_pred)}\")\n",
        "print(f\"Filas en sample_submission: {len(submission)}\")\n",
        "\n",
        "if len(y_test_pred) == len(submission):\n",
        "    submission[\"label\"] = y_test_pred\n",
        "    # Nombre de archivo actualizado\n",
        "    submission.to_csv(\"submission_logreg_pruned_1.csv\", index=False)\n",
        "    print(\"¬°Archivo 'submission_logreg_pruned_1.csv' guardado con √©xito!\")\n",
        "\n",
        "    # Vista previa\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"¬°ALERTA! Las dimensiones no coinciden.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDVYOZv_M1e9",
        "outputId": "e5f68b63-c1af-4a7c-e03c-6fad78e401e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando Regresi√≥n Log√≠stica con el dataset PREPROCESADO...\n",
            "Generando predicciones sobre el Test...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "APLICACI√ìN DE HIPERPAR√ÅMETROS √ìPTIMOS REGRESI√ìN LOG√çSTICA\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RGRTgn0Yd36v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  0.54347\n",
        "print(\"Entrenando Regresi√≥n Log√≠stica con hiperpar√°metros OPTIMIZADOS\")\n",
        "\n",
        "lr_final = LogisticRegression(\n",
        "    C=0.08,\n",
        "    solver='lbfgs',\n",
        "    class_weight=None,\n",
        "    random_state=42,\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "lr_final.fit(X_train_prep, y_train)\n",
        "\n",
        "print(\"Generando predicciones sobre el Test...\")\n",
        "y_test_pred = lr_final.predict(X_test_prep)\n",
        "\n",
        "# Verificaciones de seguridad\n",
        "print(f\"Predicciones generadas: {len(y_test_pred)}\")\n",
        "print(f\"Filas en sample_submission: {len(submission)}\")\n",
        "\n",
        "if len(y_test_pred) == len(submission):\n",
        "    submission[\"label\"] = y_test_pred\n",
        "    submission.to_csv(\"submission_logreg_pruned_opt_1.csv\", index=False)\n",
        "    print(\"¬°Archivo 'submission_logreg_pruned_opt_1.csv' guardado con √©xito!\")\n",
        "\n",
        "    # Vista previa\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"¬°ALERTA! Las dimensiones no coinciden.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvvXvSCid23l",
        "outputId": "cf5ef30b-b7b0-4fbf-91d7-c08552ba11ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando Regresi√≥n Log√≠stica con hiperpar√°metros OPTIMIZADOS\n",
            "Generando predicciones sobre el Test...\n",
            "Predicciones generadas: 184\n",
            "Filas en sample_submission: 184\n",
            "¬°Archivo 'submission_logreg_pruned_opt_1.csv' guardado con √©xito!\n",
            "   ID  label\n",
            "0   0      3\n",
            "1   1      0\n",
            "2   2      0\n",
            "3   3      2\n",
            "4   4      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## DOCUMENTACI√ìN\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dpPKh319N8fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo fue superar el estancamiento del 0.559 (Iteraci√≥n 2) eliminando el ruido que confund√≠a a los modelos geom√©tricos.\n",
        "\n",
        "### 1. üß™ Innovaci√≥n: Pipeline de \"Pruning Agresivo\"\n",
        "Se implement√≥ una selecci√≥n quir√∫rgica de variables en lugar de usar el dataset completo:\n",
        "* **Ingenier√≠a de Caracter√≠sticas:** Creaci√≥n de variables sint√©ticas cl√≠nicas (`score_respuesta_stress`, `carga_comorbilidad`).\n",
        "* **Pruning (Poda):** Eliminaci√≥n de variables originales ruidosas tras extraer su valor (`colesterol`, `azucar`, etc.).\n",
        "* **Optimizaci√≥n:** Ajuste fino del SVM (`C=1`, `gamma=0.05`) al nuevo espacio dimensional.\n",
        "\n",
        "### 2. üèÜ Resultados en Kaggle (Nuevo Techo de Cristal)\n",
        "* **ü•á SVM Pruned Optimized:** **Score 0.56521**.\n",
        "    * **Hito:** Se rompe la barrera del 0.56. Este modelo establece el **m√°ximo rendimiento obtenido en todo el proyecto hasta ahora**.\n",
        "    * *Insight:* Menos es m√°s. Al eliminar variables irrelevantes, el SVM encontr√≥ un hiperplano m√°s generalizable que con el dataset completo.\n",
        "* **ü•à SVM Pruned Base:** **0.548**. Sin ajuste de hiperpar√°metros, el pruning por s√≠ solo no basta; requiere re-sintonizaci√≥n.\n",
        "* **ü•â Regresi√≥n Log√≠stica:** **0.543**. Se mantiene estable, confirmando que ya alcanz√≥ su l√≠mite lineal.\n",
        "\n",
        "### 3. üìâ Conclusi√≥n\n",
        "El **SVM con Pruning y Optimizaci√≥n** es el modelo a batir. La limpieza de variables (Feature Selection) aport√≥ m√°s valor que la limpieza de datos de la iteraci√≥n anterior."
      ],
      "metadata": {
        "id": "Nfiz4SU7OBCa"
      }
    }
  ]
}